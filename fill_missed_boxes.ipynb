{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03ceefc6-66cd-4965-b45a-95049af39e64",
   "metadata": {},
   "source": [
    "#### Мотивация"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badb3817-c108-4612-a58d-2ba774562db3",
   "metadata": {},
   "source": [
    "В ходе изучения YOLO object tracking было выяснено, что иногда на кадрах теряются боксы с затреканым покупателем. При обучении такой кадр не попадает в обучающую выборку. Чтобы решить эту проблему, можно алгоритмически дополнять кадры потерянными боксами - а именно, брать среднее от координат последнего затреканного бокса, и первого после кадров без боксов.\n",
    "\n",
    "\n",
    "##### Про conf:\n",
    "\n",
    "Если при трекинге ставить его сильно низким, например 0.1, то сталкиваемся с новой проблемой: в качестве человека выделяются части полок с товарами в магазине, или любые другие объекты вблизи покупателя. При обучении берем один бокс с кадра, и по нему вырезаем. Поэтому боксы с мусором только зашумлят обучающую выборку"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89cb7ba-e45d-47a5-afeb-a8e2a53dc3ea",
   "metadata": {},
   "source": [
    "#### Примеры\n",
    "[Пример](https://drive.google.com/drive/u/0/folders/16RonDOowTW_iqU8cw37MXOckbxQxqb6F): видео, на котором модель YOLOv8 nano \"потеряла\" 387 боксов, на большей части которых явно есть покупатель и даже прослеживается кража."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "890bf738-19ee-4e45-b1f7-b5a0de3a2973",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c280b0e4-f095-4bc3-a70b-69ac802eacd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_video_withBoxes_path = '/home/anastasia/Desktop/MobileSSD_test/test_videos/Shoplifting7.mp4'\n",
    "example_video_raw = '/home/anastasia/Desktop/project_dataset/Shoplifting7.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12c1a903-903f-4fce-96f5-8c51c6565135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video_with_tracking(model, input_video_path, output_video_path, output_missed_path):\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "    missed_boxes_counter = 0\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        raise Exception(\"Error: Could not open video file.\")\n",
    "\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
    "    out_missed = cv2.VideoWriter(output_missed_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        results = model.track(frame, iou=0.4, conf=0.25, persist=True, imgsz=608, verbose=False, tracker=\"bytetrack.yaml\", classes=0)\n",
    "    \n",
    "        if results[0].boxes.id is not None: # this will ensure that id is not None -> exist tracks\n",
    "            boxes = results[0].boxes.xyxy.cpu().numpy().astype(int)\n",
    "            ids = results[0].boxes.id.cpu().numpy().astype(int)\n",
    "\n",
    "            for box, id in zip(boxes, ids):\n",
    "                additional_area = 1/10\n",
    "                x_min, y_min, x_max, y_max = box\n",
    "                width, height = frame_width, frame_height\n",
    "                box_width = x_max - x_min\n",
    "                box_height = y_max - y_min\n",
    "                x_min_2 = max(0, x_min - box_width*additional_area)\n",
    "                y_min_2 = max(0, y_min - box_height*additional_area)\n",
    "                x_max_2 = min(width, x_max + box_width*additional_area)\n",
    "                y_max_2 = min(height, y_max + box_height*additional_area)\n",
    "                area = (x_min_2, y_min_2, x_max_2, y_max_2)\n",
    "                cropped_img = Image.fromarray(frame, 'RGB').crop(area)\n",
    "                cropped_frame = frame[box[1]:box[3], box[0]:box[2]]\n",
    "                color = (0, 255, 255)\n",
    "                cv2.rectangle(frame, (box[0], box[1]), (box[2], box[3],), color, 2)\n",
    "                cv2.putText(\n",
    "                    frame,\n",
    "                    f\"Id {id}\",\n",
    "                    (box[0], box[1]),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.70,\n",
    "                    (0, 255, 255),\n",
    "                    2,\n",
    "                )\n",
    "        else:\n",
    "            missed_boxes_counter += 1\n",
    "            frame_missed = frame\n",
    "        \n",
    "        out.write(frame)\n",
    "        out_missed.write(frame_missed)\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    out_missed.release()\n",
    "\n",
    "    return missed_boxes_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f700467a-3ac5-4c4e-b9f4-65732c146e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLOv8n summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "387"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = YOLO('yolov8n.pt')\n",
    "model.fuse()\n",
    "sum_missed = process_video_with_tracking(model, example_video_raw,\"fill_missing_boxes/example.mp4\", 'fill_missing_boxes/missed.mp4')\n",
    "sum_missed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0ab3dcc7-4afe-49b3-9f7c-a1f7d987e6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def process_video_with_tracking_filling(model, input_video_path, output_video_path):\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        raise Exception(\"Error: Could not open video file.\")\n",
    "\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    previous_boxes = None\n",
    "    previous_ids = None\n",
    "    seek_box_frames = {}\n",
    "\n",
    "    def fill_missed_boxes(frame_count, last_boxes, next_boxes):\n",
    "        interpolated_boxes = {}\n",
    "        for box_key in last_boxes:\n",
    "            if box_key in next_boxes:\n",
    "                interpolated_boxes[box_key] = []\n",
    "                for i in range(len(next_boxes[box_key])):\n",
    "                    interpolated_coord = np.linspace(\n",
    "                        last_boxes[box_key][i], \n",
    "                        next_boxes[box_key][i], \n",
    "                        frame_count+2, \n",
    "                        dtype=int\n",
    "                    )[1:frame_count+1]\n",
    "                    interpolated_boxes[box_key].append(interpolated_coord.tolist())\n",
    "        return interpolated_boxes\n",
    "\n",
    "    frame_idx = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        results = model.track(frame, iou=0.4, conf=0.25, persist=True, imgsz=608, verbose=False, tracker=\"bytetrack.yaml\", classes=0)\n",
    "        frame_boxes = {}\n",
    "        if results[0].boxes.id is not None:\n",
    "            boxes = results[0].boxes.xyxy.cpu().numpy().astype(int)\n",
    "            ids = results[0].boxes.id.cpu().numpy().astype(int)\n",
    "            for box, id in zip(boxes, ids):\n",
    "                frame_boxes[id] = box\n",
    "                additional_area = 1/10\n",
    "                x_min, y_min, x_max, y_max = box\n",
    "                width, height = frame_width, frame_height\n",
    "                box_width = x_max - x_min\n",
    "                box_height = y_max - y_min\n",
    "                x_min_2 = max(0, x_min - box_width * additional_area)\n",
    "                y_min_2 = max(0, y_min - box_height * additional_area)\n",
    "                x_max_2 = min(width, x_max + box_width * additional_area)\n",
    "                y_max_2 = min(height, y_max + box_height * additional_area)\n",
    "                box_area = (x_min_2, y_min_2, x_max_2, y_max_2)\n",
    "                cropped_img = Image.fromarray(frame, 'RGB').crop(box_area)\n",
    "                color = (0, 255, 255)\n",
    "                cv2.rectangle(frame, (box[0], box[1]), (box[2], box[3]), color, 2)\n",
    "                cv2.putText(\n",
    "                    frame,\n",
    "                    f\"Id {id}\",\n",
    "                    (box[0], box[1]),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.70,\n",
    "                    (0, 255, 255),\n",
    "                    2,\n",
    "                )      \n",
    "        if len(frame_boxes) == 0:\n",
    "            seek_box_frames[frame_idx] = {\n",
    "                \"frame\": frame,\n",
    "                \"prev_boxes\": previous_boxes,\n",
    "                \"next_boxes\": None\n",
    "            }\n",
    "        else:\n",
    "            if not previous_boxes:\n",
    "                previous_boxes = frame_boxes\n",
    "                out.write(frame)\n",
    "            else:\n",
    "                if seek_box_frames:\n",
    "                    min_seek_idx = min(seek_box_frames)\n",
    "                    frame_interval = frame_idx - min_seek_idx - 1\n",
    "\n",
    "                    if frame_interval > 0:\n",
    "                        interpolated_boxes = fill_missed_boxes(frame_interval, previous_boxes, frame_boxes)\n",
    "                        for missing_frame_idx in range(frame_interval):\n",
    "                            seek_frame_idx = min_seek_idx + missing_frame_idx + 1\n",
    "                            if seek_frame_idx in seek_box_frames:\n",
    "                                seek_frame = seek_box_frames.pop(seek_frame_idx)[\"frame\"]\n",
    "\n",
    "                                for id in interpolated_boxes.keys():\n",
    "                                    valid_id = id.item()\n",
    "                                    # box = interpolated_boxes[valid_id][:, missing_frame_idx]\n",
    "                                    box = np.array(interpolated_boxes[id])[:, missing_frame_idx]\n",
    "                                    color = (255, 0, 0)\n",
    "                                    cv2.rectangle(seek_frame, (box[0], box[1]), (box[2], box[3]), color, 2)\n",
    "                                    cv2.putText(\n",
    "                                        seek_frame,\n",
    "                                        f\"Id {id}\",\n",
    "                                        (box[0], box[1]),\n",
    "                                        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                                        0.70,\n",
    "                                        color,\n",
    "                                        2,\n",
    "                                    )\n",
    "                                out.write(seek_frame)\n",
    "                previous_boxes = frame_boxes\n",
    "                out.write(frame)\n",
    "        \n",
    "        frame_idx += 1\n",
    "\n",
    "    while seek_box_frames:\n",
    "        seek_frame = seek_box_frames.popitem()[1][\"frame\"]\n",
    "        out.write(seek_frame)\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b6baa324-f3be-4af5-a5aa-b2a5fa33898d",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_video_with_tracking_filling(model, example_video_raw,\"fill_missing_boxes/example_test_filling.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a502ec-8e94-41ba-a8cc-99a6ce8a2e88",
   "metadata": {},
   "source": [
    "TO-DO: добавить паддинг"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9243aa8-37ee-4107-b768-0d8e7a457ee4",
   "metadata": {},
   "source": [
    "TO-DO: заполнять только если координаты соседних непропущенных боксов не сильно отличаются"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
