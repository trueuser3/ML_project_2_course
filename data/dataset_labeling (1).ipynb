{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e395ec3c-3b0c-4dac-b188-08295046d90a",
   "metadata": {},
   "source": [
    "*Берутся видео с датасета `https://disk.yandex.ru/d/_vjY_E84Bs1p-Q`, обрезается лишнее(чтобы не было очень сильного дисбаланса классов), бьются на кадры, затем вручную в csv файл записываются метки каждого кадра, а также координаты бокса с человеком(если вдруг понадобится)*\n",
    "Получившийся датасет лежит в `https://drive.google.com/drive/folders/1YTx-Rj6D7dj0WFRjYTJHJ6_gOz8KsCh5`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acff7f3-772d-44d9-ad35-cff28f12d1d9",
   "metadata": {},
   "source": [
    "*Обрезаем видео*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2544d58c-cbcb-44e9-9f71-e80bc2890431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: moviepy in /home/anastasia/.local/lib/python3.10/site-packages (1.0.3)\n",
      "Requirement already satisfied: decorator<5.0,>=4.0.2 in /home/anastasia/.local/lib/python3.10/site-packages (from moviepy) (4.4.2)\n",
      "Requirement already satisfied: proglog<=1.0.0 in /home/anastasia/.local/lib/python3.10/site-packages (from moviepy) (0.1.10)\n",
      "Requirement already satisfied: requests<3.0,>=2.8.1 in /home/anastasia/.local/lib/python3.10/site-packages (from moviepy) (2.31.0)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /home/anastasia/.local/lib/python3.10/site-packages (from moviepy) (4.66.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/anastasia/.local/lib/python3.10/site-packages (from moviepy) (1.26.0)\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in /home/anastasia/.local/lib/python3.10/site-packages (from moviepy) (2.34.0)\n",
      "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /home/anastasia/.local/lib/python3.10/site-packages (from moviepy) (0.4.9)\n",
      "Requirement already satisfied: pillow>=8.3.2 in /home/anastasia/.local/lib/python3.10/site-packages (from imageio<3.0,>=2.5->moviepy) (10.3.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from imageio-ffmpeg>=0.2.0->moviepy) (59.6.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/anastasia/.local/lib/python3.10/site-packages (from requests<3.0,>=2.8.1->moviepy) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/anastasia/.local/lib/python3.10/site-packages (from requests<3.0,>=2.8.1->moviepy) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2020.6.20)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --trusted-host pypi.python.org moviepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47a6a8a9-3716-4f0d-a29f-e788721c75b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: imageio-ffmpeg in /home/anastasia/.local/lib/python3.10/site-packages (0.4.9)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from imageio-ffmpeg) (59.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install imageio-ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75ab629f-3727-4181-9a4c-2421364d2377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n"
     ]
    }
   ],
   "source": [
    "from moviepy.video.io.ffmpeg_tools import ffmpeg_extract_subclip\n",
    "start_time = 20 # seconds\n",
    "end_time = 26\n",
    "ffmpeg_extract_subclip(\"Shoplifting1.mp4\", start_time, end_time, targetname=\"test.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfd03939-1e95-4a39-ae03-a8741ddaf47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_cut(video, start_time, end_time, count): # save cut video in the same directory\n",
    "    ffmpeg_extract_subclip(video, start_time, end_time, targetname=\"Shoplifting\"+str(count)+\".mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3854d65f-e0b7-4e0c-b73b-1338620e7d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n"
     ]
    }
   ],
   "source": [
    "video_cut(\"/home/anastasia/Downloads/Shoplifting053_x264.mp4\", 40, 48, 48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7dc0052-d3a3-41bb-8b92-e4efa6ec02e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import VideoFileClip, vfx\n",
    "\n",
    "def speed_up_video(input_video_path):\n",
    "    output_path = input_video_path.replace('.mp4', '_speed.mp4')  \n",
    "    video = VideoFileClip(input_video_path)\n",
    "    speedup_video = video.fx(vfx.speedx, 1.5)  \n",
    "    speedup_video.write_videofile(output_path, codec='libx264') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14904dad-4198-4924-8eed-debdf6f91169",
   "metadata": {},
   "source": [
    "*Получаем кадры*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "dcc4943f-685a-4680-9e35-769811d42013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "from ultralytics.utils.plotting import Annotator\n",
    "from PIL import Image\n",
    "def receive_frames(video_path, counter):\n",
    "    #video_path = 'Shoplifting5.mp4' \n",
    "    output_folder = 'output_frames_1'\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_count = 0\n",
    "    \n",
    "    #counter = 5\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame_count += 1\n",
    "        frame_name = f'frame_{counter}_{frame_count}.jpg'\n",
    "        cv2.imwrite(os.path.join(output_folder, frame_name), frame)\n",
    "    \n",
    "    cap.release()\n",
    "    #cv2.destroyAllWindows()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "603e0f28-6549-4090-9bf5-2d90910c4ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(23, 49):\n",
    "    video_name = f\"Shoplifting{i}.mp4\"\n",
    "    receive_frames(video_name, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3c1ef058-a2ce-4101-af4c-81f00883328e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_frames = pd.DataFrame(columns = ['file_name', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b6c1f809-3390-4eb5-a15f-060584ae285b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_frames = pd.read_csv('metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef360184-af09-4940-9880-16c924c76816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>frame_2_105.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>frame_7_484.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>frame_8_87.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>frame_7_112.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>frame_7_388.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11975</th>\n",
       "      <td>frame_34_54.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11976</th>\n",
       "      <td>frame_35_79.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11977</th>\n",
       "      <td>frame_36_58.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11978</th>\n",
       "      <td>frame_37_112.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11979</th>\n",
       "      <td>frame_40_224.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11980 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              file_name  labels\n",
       "0       frame_2_105.jpg       1\n",
       "1       frame_7_484.jpg       1\n",
       "2        frame_8_87.jpg       1\n",
       "3       frame_7_112.jpg       0\n",
       "4       frame_7_388.jpg       0\n",
       "...                 ...     ...\n",
       "11975   frame_34_54.jpg       1\n",
       "11976   frame_35_79.jpg       0\n",
       "11977   frame_36_58.jpg       1\n",
       "11978  frame_37_112.jpg       0\n",
       "11979  frame_40_224.jpg       0\n",
       "\n",
       "[11980 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015a1ab7-cb60-4c73-9f27-709e109b95f6",
   "metadata": {},
   "source": [
    "*Добавляем в датафрейм новые кадры*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "807ac037-d797-4afb-8c64-093f7b2b980c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "image_dir = 'output_frames_1'\n",
    "image_files = os.listdir(image_dir)\n",
    "new_rows = []\n",
    "for image_file in image_files:\n",
    "        file_name = os.path.basename(image_file)\n",
    "        if file_name not in df_frames['file_name'].values:\n",
    "            new_row = {'file_name': file_name, 'labels': 0}\n",
    "            new_rows.append({'file_name': file_name, 'labels': 0})\n",
    "df_frames = pd.concat([df_frames, pd.DataFrame(new_rows)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f7397b85-8a71-4d5c-8982-89d587aebab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11980 entries, 0 to 11979\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   file_name  11980 non-null  object\n",
      " 1   labels     11980 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 187.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df_frames.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7c8a4a-e030-4e29-983f-ad3085cf5298",
   "metadata": {},
   "source": [
    "*Проставляем метки на кадры краж*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "d5b8b733-5680-4292-9ab6-a76d31290377",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_label(number_video, start_frame, finish_frame):\n",
    "    for i in range(start_frame, finish_frame + 1):\n",
    "        df_frames.loc[df_frames['file_name'] == f\"frame_{number_video}_{i}.jpg\", 'labels'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "60b87497-b440-44da-9898-08887208aa50",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_label(48, 96, 131)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f68172-e076-4bc6-8c68-a52b2e25ee18",
   "metadata": {},
   "source": [
    "*Просмотров кадров*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "b35b2379-8c9c-4640-af71-43d20f639e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import time\n",
    "def show_video_frames(number_video):\n",
    "    path = 'frames'\n",
    "    images = [img for img in os.listdir(path) if img.endswith(\".jpg\") and f\"frame_{number_video}\" in img]\n",
    "    #images.sort()\n",
    "    \n",
    "    for img in images:\n",
    "        image_path = os.path.join(path, img)\n",
    "        image = Image.open(image_path)\n",
    "        image.show()\n",
    "        time.sleep(0.25)\n",
    "        image.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "bf5bce5a-30b9-43f3-bf28-7d04ed5b3813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: ylabel='Frequency'>"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "df_frames['labels'].plot(kind='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "ded8c824-a1a8-4f9a-942d-c3cf33b00943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4153"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df_frames['labels'] == 1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "aa67a7f4-1342-42ec-b5da-da3caf02c76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_frames.to_csv('metadata.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225d3b53-86fc-4c9a-a497-54e2d384a4d2",
   "metadata": {},
   "source": [
    "*Вспомогательные функции для обрезки по yolo боксам*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f56adff1-0cb3-4f27-b349-e9524574ff41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def crop_image(image, box):\n",
    "    additional_area = 1/5\n",
    "    x_min, y_min, x_max, y_max = box\n",
    "    width, height = image.size\n",
    "    box_width = x_max - x_min\n",
    "    box_height = y_max - y_min\n",
    "    x_min_2 = max(0, x_min - box_width*additional_area)\n",
    "    y_min_2 = max(0, y_min - box_height*additional_area)\n",
    "    x_max_2 = min(width, x_max + box_width*additional_area)\n",
    "    y_max_2 = min(height, y_max + box_height*additional_area)\n",
    "    area = (x_min_2, y_min_2, x_max_2, y_max_2)\n",
    "    cropped_img = image.crop(area)\n",
    "    return cropped_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eee1702f-dc97-447d-9d69-939bd3deaa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def recieve_box(image, frame):\n",
    "    model = YOLO('yolov8n.pt')\n",
    "    model.classes = [0]\n",
    "    results = model(source = image, classes=0, show = False, imgsz=640, conf=0.3, iou=0.4, save = False)\n",
    "    boxes = results[0].boxes.xyxy.cpu().numpy().astype(int)\n",
    "    cropped_images = []\n",
    "    for box in boxes:\n",
    "        cropped_images.append(crop_image(image, box))\n",
    "    save_dir = 'frames_cropped'\n",
    "    counter = 1\n",
    "    for im in cropped_images:\n",
    "        save_path = os.path.join(save_dir, f\"_cropped_{counter}.jpg\")\n",
    "        im.save(save_path)\n",
    "        counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ea0afd9e-a4c3-4d38-b71a-da1b7048376a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 7 persons, 51.6ms\n",
      "Speed: 12.1ms preprocess, 51.6ms inference, 8.7ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    }
   ],
   "source": [
    "path_to_image = 'frames/frame_30_106.jpg'\n",
    "image = Image.open(path_to_image)\n",
    "recieve_box(image, path_to_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac98d7e1-052d-4e16-83ec-2874d3bdfa15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1cf627eb-80e9-486b-8753-192a60ac79e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for im in res:\n",
    "    im.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca999a3-3ceb-495c-920f-62103e7c2721",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from ultralytics.utils.plotting import Annotator\n",
    "\n",
    "model = YOLO('yolov8n.pt')\n",
    "model.classes = [0]\n",
    "image_path = '/content/drive/MyDrive/yolo_dataset/test/images/cam-pos-video-five_0mABuXZe_mp4-36_jpg.rf.9f17c4aa766ded211ed3a70d1728403f.jpg'\n",
    "results = model(source = image_path, classes=0, show = False, imgsz=640, conf=0.3, iou=0.4, save = True)\n",
    "\n",
    "import os\n",
    "image_path = '/content/drive/MyDrive/yolo_dataset/test/images/cam-pos-video-five_0mABuXZe_mp4-36_jpg.rf.9f17c4aa766ded211ed3a70d1728403f.jpg'\n",
    "boxes = results[0].boxes.xyxy.cpu().numpy().astype(int)\n",
    "save_dir = 'runs/detect/predict18'\n",
    "for box in boxes:\n",
    "    x_min, y_min, x_max, y_max = box\n",
    "    #print(f\"Box coordinates: ({x_min}, {y_min}), ({x_max}, {y_max})\")\n",
    "    img = Image.open(image_path)\n",
    "    width, height = img.size\n",
    "    box_width = x_max - x_min\n",
    "    box_height = y_max - y_min\n",
    "    x_min_2 = max(0, x_min - box_width/3)\n",
    "    y_min_2 = max(0, y_min - box_height/3)\n",
    "    x_max_2 = min(width, x_max+ box_width/3)\n",
    "    y_max_2 = min(height, y_max + box_height/3)\n",
    "    area = (x_min_2, y_min_2, x_max_2, y_max_2)\n",
    "    cropped_img = img.crop(area)\n",
    "    save_path = os.path.join(save_dir, \"cropped_.jpg\")\n",
    "    cropped_img.save(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc174292-b30f-42aa-b795-1163be09fdfa",
   "metadata": {},
   "source": [
    "*Функция для трекинга человека на видео. Сохраняет видео с людьми в боксах и помечает их id*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c8f7ca48-0152-4f18-9780-277b970fc7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "def process_video_with_tracking(model, input_video_path, output_video_path):\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        raise Exception(\"Error: Could not open video file.\")\n",
    "\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        results = model.track(frame, iou=0.4, conf=0.25, persist=True, imgsz=608, verbose=False, tracker=\"bytetrack.yaml\", classes=0)\n",
    "    \n",
    "        if results[0].boxes.id != None: # this will ensure that id is not None -> exist tracks\n",
    "            boxes = results[0].boxes.xyxy.cpu().numpy().astype(int)\n",
    "            ids = results[0].boxes.id.cpu().numpy().astype(int)\n",
    "\n",
    "            for box, id in zip(boxes, ids):\n",
    "                additional_area = 1/10\n",
    "                x_min, y_min, x_max, y_max = box\n",
    "                width, height = frame_width, frame_height\n",
    "                box_width = x_max - x_min\n",
    "                box_height = y_max - y_min\n",
    "                x_min_2 = max(0, x_min - box_width*additional_area)\n",
    "                y_min_2 = max(0, y_min - box_height*additional_area)\n",
    "                x_max_2 = min(width, x_max + box_width*additional_area)\n",
    "                y_max_2 = min(height, y_max + box_height*additional_area)\n",
    "                area = (x_min_2, y_min_2, x_max_2, y_max_2)\n",
    "                cropped_img = Image.fromarray(frame, 'RGB').crop(area)\n",
    "                cropped_frame = frame[box[1]:box[3], box[0]:box[2]]\n",
    "                color = (0, 255, 255)\n",
    "                cv2.rectangle(frame, (box[0], box[1]), (box[2], box[3],), color, 2)\n",
    "                cv2.putText(\n",
    "                    frame,\n",
    "                    f\"Id {id}\",\n",
    "                    (box[0], box[1]),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.70,\n",
    "                    (0, 255, 255),\n",
    "                    2,\n",
    "                )\n",
    "\n",
    "        \n",
    "        out.write(frame)\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a56390-5eea-4600-bdb1-1da5c8332a12",
   "metadata": {},
   "source": [
    "*Функция, которая получает видео с n людьми в кадре и записывает в директорию n видео, где k-ое видео представляет собой исходное, обрезанное так, чтобы в кадре был только k-ый человек*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "809b6151-b96a-4c25-af3f-4f613b6a87d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "\n",
    "def process_video_individual_padding(model, input_video_path, output_dir):\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "    if not cap.isOpened():\n",
    "        raise Exception(\"Error: Could not open video file.\")\n",
    "\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    writers = {}\n",
    "    writers_sizes = {}\n",
    "    frame_count = 0\n",
    "    additional_area = 1/5\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        results = model.track(frame, iou=0.4, conf=0.3, persist=True, imgsz=608, verbose=False, tracker=\"bytetrack.yaml\", classes=0)\n",
    "        frame_count += 1\n",
    "\n",
    "        if hasattr(results[0].boxes, 'id') and results[0].boxes.id is not None:\n",
    "            boxes = results[0].boxes.xyxy.cpu().numpy().astype(int)\n",
    "            ids = results[0].boxes.id.cpu().numpy().astype(int)\n",
    "\n",
    "            for box, person_id in zip(boxes, ids):\n",
    "                x_min, y_min, x_max, y_max = box\n",
    "                pad_width = (x_max - x_min) * additional_area\n",
    "                pad_height = (y_max - y_min) * additional_area\n",
    "                x_min_pad = max(0, int(x_min - pad_width))\n",
    "                y_min_pad = max(0, int(y_min - pad_height))\n",
    "                x_max_pad = min(frame_width, int(x_max + pad_width))\n",
    "                y_max_pad = min(frame_height, int(y_max + pad_height))\n",
    "\n",
    "                cropped_frame = frame[y_min_pad:y_max_pad, x_min_pad:x_max_pad]\n",
    "                if person_id not in writers:\n",
    "                    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "                    output_size = (x_max_pad - x_min_pad, y_max_pad - y_min_pad)\n",
    "                    writer = cv2.VideoWriter(f\"{output_dir}/person_{person_id}.mp4\", fourcc, fps, output_size)\n",
    "                    writers[person_id] = writer\n",
    "                    writers_sizes[person_id] = output_size\n",
    "                    \n",
    "                else:\n",
    "                    desired_size = writers_sizes[person_id]\n",
    "                    resized_frame = cv2.resize(cropped_frame, desired_size)\n",
    "\n",
    "                    if resized_frame.size != 0:\n",
    "                        writers[person_id].write(resized_frame)\n",
    "\n",
    "        if frame_count % 100 == 0:\n",
    "            print(f\"Processed {frame_count} frames.\")\n",
    "\n",
    "\n",
    "    cap.release()\n",
    "    for writer in writers.values():\n",
    "        writer.release()\n",
    "\n",
    "    writers.clear()\n",
    "    writers_sizes.clear()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08df1d4-5000-402d-87e4-771e46aa1e1d",
   "metadata": {},
   "source": [
    "*Обрезаем видео по конкретному боксу(человека с id=track_id)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "94d5cfae-fb6b-458e-8dd2-96783072fd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "def process_single_track_video(model, input_video_path, output_video_path, track_id):\n",
    "    \n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "    if not cap.isOpened():\n",
    "        raise Exception(\"Error: Could not open video file.\")\n",
    "    \n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    \n",
    "    out = None\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        results = model.track(frame, iou=0.4, conf=0.3, persist=True, imgsz=608, verbose=False, tracker=\"bytetrack.yaml\", classes=0)\n",
    "        \n",
    "        if hasattr(results[0].boxes, 'id') and results[0].boxes.id != None:\n",
    "            boxes = results[0].boxes.xyxy.cpu().numpy().astype(int)\n",
    "            ids = results[0].boxes.id.cpu().numpy().astype(int)\n",
    "\n",
    "            for box, id in zip(boxes, ids):\n",
    "                if id == track_id:\n",
    "                    x_min, y_min, x_max, y_max = box\n",
    "                    if out is None:\n",
    "                        width = x_max - x_min\n",
    "                        height = y_max - y_min\n",
    "                        if width > 0 and height > 0:\n",
    "                            out = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
    "                            fixed_size = (int(width), int(height))\n",
    "                    try:\n",
    "                        x_min = max(x_min, 0)\n",
    "                        y_min = max(y_min, 0)\n",
    "                        x_max = min(frame_width, x_max)\n",
    "                        y_max = min(frame_height, y_max)\n",
    "                        cropped_frame = frame[y_min:y_max, x_min:x_max]\n",
    "                        if cropped_frame.shape[1] != fixed_size[0] or cropped_frame.shape[0] != fixed_size[1]:\n",
    "                            cropped_frame = cv2.resize(cropped_frame, (fixed_size[0], fixed_size[1]))\n",
    "                        out.write(cropped_frame)\n",
    "                    except Exception as e:\n",
    "                        print(\"Error managing frame size:\", e)\n",
    "    \n",
    "    cap.release()\n",
    "    if out is not None:\n",
    "        out.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb27b417-d1b7-49a9-a048-c50e108d200a",
   "metadata": {},
   "source": [
    "*examples of usages*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3766fee2-0ef2-408a-b23e-0a895f5021f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLOv8m summary (fused): 218 layers, 25886080 parameters, 0 gradients, 78.9 GFLOPs\n",
      "Processed 100 frames.\n"
     ]
    }
   ],
   "source": [
    "model = YOLO('yolov8m.pt')\n",
    "model.fuse()\n",
    "results = process_video_individual_padding(model, \"Shoplifting1.mp4\", output_dir=\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ba5e011b-5fd0-4f94-ae03-ca2920145769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLOv8n summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs\n"
     ]
    }
   ],
   "source": [
    "model = YOLO('yolov8n.pt')\n",
    "model.fuse()\n",
    "results = process_video_with_tracking(model, \"Shoplifting7.mp4\", output_video_path=\"output/output_sample7.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "21fa00b9-a6c1-4208-8b95-8699e0a29803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLOv8m summary (fused): 218 layers, 25886080 parameters, 0 gradients, 78.9 GFLOPs\n"
     ]
    }
   ],
   "source": [
    "model = YOLO('yolov8m.pt')\n",
    "model.fuse()\n",
    "process_single_track_video(model, \"videos_with_boxes/focused_Shoplifting6.mp4\", \"output_sample6.mp4\", 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5735c743-21e0-4b6b-8d5c-c5cffbe5f63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "process_single_track_video(model, \"Shoplifting46.mp4\", \"output_sample46.mp4\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "62efecfc-f39b-49f7-9ed7-e458db4a3076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/anastasia/Desktop/project_dataset'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "013a05ed-2fe5-4506-84e7-5af7387b5031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/anastasia/Desktop/project_dataset'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8b819c51-8c91-4819-84d8-6bdfdfc1dc1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLOv8m summary (fused): 218 layers, 25886080 parameters, 0 gradients, 78.9 GFLOPs\n",
      "Processing video: Shoplifting2.mp4\n",
      "Processing video: Shoplifting3.mp4\n",
      "Processing video: Shoplifting4.mp4\n",
      "Processing video: Shoplifting5.mp4\n",
      "Processing video: Shoplifting6.mp4\n",
      "Processing video: Shoplifting7.mp4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m src_directory \u001b[38;5;241m=\u001b[39m cwd\n\u001b[1;32m     23\u001b[0m res_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvideos_with_boxes\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 25\u001b[0m \u001b[43mprocess_all_videos\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mres_directory\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[49], line 16\u001b[0m, in \u001b[0;36mprocess_all_videos\u001b[0;34m(directory_path, output_directory)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing video: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvideo_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 16\u001b[0m     \u001b[43mprocess_video_with_tracking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_video_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to process \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvideo_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[43], line 22\u001b[0m, in \u001b[0;36mprocess_video_with_tracking\u001b[0;34m(model, input_video_path, output_video_path)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ret:\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miou\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpersist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m608\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbytetrack.yaml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(results[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mboxes, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m results[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mboxes\u001b[38;5;241m.\u001b[39mid \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;66;03m# this will ensure that id is not None -> exist tracks\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     boxes \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mboxes\u001b[38;5;241m.\u001b[39mxyxy\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/engine/model.py:479\u001b[0m, in \u001b[0;36mModel.track\u001b[0;34m(self, source, stream, persist, **kwargs)\u001b[0m\n\u001b[1;32m    477\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# batch-size 1 for tracking in videos\u001b[39;00m\n\u001b[1;32m    478\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrack\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 479\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/engine/model.py:439\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[0;32m--> 439\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/engine/predictor.py:168\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[0;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:35\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m---> 35\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/engine/predictor.py:248\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[0;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m--> 248\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39membed:\n\u001b[1;32m    250\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m [preds] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(preds, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m preds  \u001b[38;5;66;03m# yield embedding tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/engine/predictor.py:142\u001b[0m, in \u001b[0;36mBasePredictor.inference\u001b[0;34m(self, im, *args, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs inference on a given image using the specified model and arguments.\"\"\"\u001b[39;00m\n\u001b[1;32m    137\u001b[0m visualize \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    138\u001b[0m     increment_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_dir \u001b[38;5;241m/\u001b[39m Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mstem, mkdir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mvisualize \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type\u001b[38;5;241m.\u001b[39mtensor)\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    141\u001b[0m )\n\u001b[0;32m--> 142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/nn/autobackend.py:425\u001b[0m, in \u001b[0;36mAutoBackend.forward\u001b[0;34m(self, im, augment, visualize, embed)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_module:\n\u001b[0;32m--> 425\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;66;03m# TorchScript\u001b[39;00m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjit:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/nn/tasks.py:89\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/nn/tasks.py:107\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[0;34m(self, x, profile, visualize, augment, embed)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_augment(x)\n\u001b[0;32m--> 107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/nn/tasks.py:128\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[0;34m(self, x, profile, visualize, embed)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[0;32m--> 128\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[1;32m    129\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(x \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/nn/modules/block.py:232\u001b[0m, in \u001b[0;36mC2f.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    230\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv1(x)\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    231\u001b[0m y\u001b[38;5;241m.\u001b[39mextend(m(y[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mm)\n\u001b[0;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/nn/modules/conv.py:54\u001b[0m, in \u001b[0;36mConv.forward_fuse\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_fuse\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     53\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Perform transposed convolution of 2D data.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "def process_all_videos(directory_path, output_directory):\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "    model = YOLO('yolov8m.pt')\n",
    "    model.fuse()\n",
    "    for i in range(2, 49):\n",
    "        if i not in [42, 43, 44, 45]:\n",
    "            video_filename = f\"Shoplifting{i}.mp4\"\n",
    "            video_path = os.path.join(directory_path, video_filename)\n",
    "            if os.path.exists(video_path):\n",
    "                output_video_path = os.path.join(output_directory, f\"focused_{video_filename}\")\n",
    "                print(f\"Processing video: {video_filename}\")\n",
    "                try:\n",
    "                    process_video_with_tracking(model, video_path, output_video_path)\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to process {video_filename}: {str(e)}\")\n",
    "            else:\n",
    "                print(f\"File {video_filename} does not exist in the directory\")\n",
    "    \n",
    "src_directory = cwd\n",
    "res_directory = \"videos_with_boxes\"\n",
    "\n",
    "process_all_videos(src_directory, res_directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
