{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e395ec3c-3b0c-4dac-b188-08295046d90a",
   "metadata": {},
   "source": [
    "*Берутся видео с датасета `https://disk.yandex.ru/d/_vjY_E84Bs1p-Q`, обрезается лишнее(чтобы не было очень сильного дисбаланса классов), бьются на кадры, затем вручную в csv файл записываются метки каждого кадра, а также координаты бокса с человеком(если вдруг понадобится)*\n",
    "Получившийся датасет лежит в `https://drive.google.com/drive/folders/1YTx-Rj6D7dj0WFRjYTJHJ6_gOz8KsCh5`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acff7f3-772d-44d9-ad35-cff28f12d1d9",
   "metadata": {},
   "source": [
    "*Обрезаем видео*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2544d58c-cbcb-44e9-9f71-e80bc2890431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: moviepy in /home/anastasia/.local/lib/python3.10/site-packages (1.0.3)\n",
      "Requirement already satisfied: decorator<5.0,>=4.0.2 in /home/anastasia/.local/lib/python3.10/site-packages (from moviepy) (4.4.2)\n",
      "Requirement already satisfied: proglog<=1.0.0 in /home/anastasia/.local/lib/python3.10/site-packages (from moviepy) (0.1.10)\n",
      "Requirement already satisfied: requests<3.0,>=2.8.1 in /home/anastasia/.local/lib/python3.10/site-packages (from moviepy) (2.31.0)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /home/anastasia/.local/lib/python3.10/site-packages (from moviepy) (4.66.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/anastasia/.local/lib/python3.10/site-packages (from moviepy) (1.26.0)\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in /home/anastasia/.local/lib/python3.10/site-packages (from moviepy) (2.34.0)\n",
      "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /home/anastasia/.local/lib/python3.10/site-packages (from moviepy) (0.4.9)\n",
      "Requirement already satisfied: pillow>=8.3.2 in /home/anastasia/.local/lib/python3.10/site-packages (from imageio<3.0,>=2.5->moviepy) (10.3.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from imageio-ffmpeg>=0.2.0->moviepy) (59.6.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/anastasia/.local/lib/python3.10/site-packages (from requests<3.0,>=2.8.1->moviepy) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/anastasia/.local/lib/python3.10/site-packages (from requests<3.0,>=2.8.1->moviepy) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2020.6.20)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --trusted-host pypi.python.org moviepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47a6a8a9-3716-4f0d-a29f-e788721c75b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: imageio-ffmpeg in /home/anastasia/.local/lib/python3.10/site-packages (0.4.9)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from imageio-ffmpeg) (59.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install imageio-ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75ab629f-3727-4181-9a4c-2421364d2377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n"
     ]
    }
   ],
   "source": [
    "from moviepy.video.io.ffmpeg_tools import ffmpeg_extract_subclip\n",
    "start_time = 20 # seconds\n",
    "end_time = 26\n",
    "ffmpeg_extract_subclip(\"Shoplifting1.mp4\", start_time, end_time, targetname=\"test.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfd03939-1e95-4a39-ae03-a8741ddaf47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_cut(video, start_time, end_time, count): # save cut video in the same directory\n",
    "    ffmpeg_extract_subclip(video, start_time, end_time, targetname=\"Shoplifting\"+str(count)+\".mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3854d65f-e0b7-4e0c-b73b-1338620e7d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n"
     ]
    }
   ],
   "source": [
    "video_cut(\"/home/anastasia/Downloads/Shoplifting053_x264.mp4\", 40, 48, 48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7dc0052-d3a3-41bb-8b92-e4efa6ec02e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import VideoFileClip, vfx\n",
    "\n",
    "def speed_up_video(input_video_path):\n",
    "    output_path = input_video_path.replace('.mp4', '_speed.mp4')  \n",
    "    video = VideoFileClip(input_video_path)\n",
    "    speedup_video = video.fx(vfx.speedx, 1.5)  \n",
    "    speedup_video.write_videofile(output_path, codec='libx264') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14904dad-4198-4924-8eed-debdf6f91169",
   "metadata": {},
   "source": [
    "*Получаем кадры*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dcc4943f-685a-4680-9e35-769811d42013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "from ultralytics.utils.plotting import Annotator\n",
    "from PIL import Image\n",
    "def receive_frames(video_path, counter):\n",
    "    output_folder = 'frames'\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_count = 0\n",
    "    \n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame_count += 1\n",
    "        frame_name = f'frame_{counter}_{frame_count}.jpg'\n",
    "        cv2.imwrite(os.path.join(output_folder, frame_name), frame)\n",
    "    \n",
    "    cap.release()\n",
    "    #cv2.destroyAllWindows()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "603e0f28-6549-4090-9bf5-2d90910c4ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(42, 46):\n",
    "    video_name = f\"Shoplifting{i}.mp4\"\n",
    "    receive_frames(video_name, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5038f51b-cf6b-458d-98f3-0fe5d925bbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "receive_frames('Shoplifting9.mp4', 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3c1ef058-a2ce-4101-af4c-81f00883328e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_frames = pd.DataFrame(columns = ['file_name', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b6c1f809-3390-4eb5-a15f-060584ae285b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_frames = pd.read_csv('metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ef360184-af09-4940-9880-16c924c76816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>frame_2_105.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>frame_7_484.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>frame_8_87.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>frame_7_112.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>frame_7_388.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11975</th>\n",
       "      <td>frame_34_54.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11976</th>\n",
       "      <td>frame_35_79.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11977</th>\n",
       "      <td>frame_36_58.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11978</th>\n",
       "      <td>frame_37_112.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11979</th>\n",
       "      <td>frame_40_224.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11980 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              file_name  labels\n",
       "0       frame_2_105.jpg       1\n",
       "1       frame_7_484.jpg       1\n",
       "2        frame_8_87.jpg       1\n",
       "3       frame_7_112.jpg       0\n",
       "4       frame_7_388.jpg       0\n",
       "...                 ...     ...\n",
       "11975   frame_34_54.jpg       1\n",
       "11976   frame_35_79.jpg       0\n",
       "11977   frame_36_58.jpg       1\n",
       "11978  frame_37_112.jpg       0\n",
       "11979  frame_40_224.jpg       0\n",
       "\n",
       "[11980 rows x 2 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5325f6d-3c99-4176-960b-7b451cf6d3bb",
   "metadata": {},
   "source": [
    "*Удаление всех кадров данного видео*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "42afffd1-ac36-4a03-97c9-1d4fd7185295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>frame_7_484.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>frame_8_87.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>frame_7_112.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>frame_7_388.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>frame_7_152.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11975</th>\n",
       "      <td>frame_34_54.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11976</th>\n",
       "      <td>frame_35_79.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11977</th>\n",
       "      <td>frame_36_58.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11978</th>\n",
       "      <td>frame_37_112.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11979</th>\n",
       "      <td>frame_40_224.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10934 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              file_name  labels\n",
       "1       frame_7_484.jpg       1\n",
       "2        frame_8_87.jpg       1\n",
       "3       frame_7_112.jpg       0\n",
       "4       frame_7_388.jpg       0\n",
       "5       frame_7_152.jpg       0\n",
       "...                 ...     ...\n",
       "11975   frame_34_54.jpg       1\n",
       "11976   frame_35_79.jpg       0\n",
       "11977   frame_36_58.jpg       1\n",
       "11978  frame_37_112.jpg       0\n",
       "11979  frame_40_224.jpg       0\n",
       "\n",
       "[10934 rows x 2 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_frames = df_frames[~df_frames.file_name.str.contains(f\"_{16}_\")]\n",
    "df_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d87cd4f9-1784-4031-8b49-e0d6e9d15026",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>frame_7_484.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>frame_8_87.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>frame_7_112.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>frame_7_388.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>frame_7_152.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11975</th>\n",
       "      <td>frame_34_54.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11976</th>\n",
       "      <td>frame_35_79.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11977</th>\n",
       "      <td>frame_36_58.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11978</th>\n",
       "      <td>frame_37_112.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11979</th>\n",
       "      <td>frame_40_224.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11793 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              file_name  labels\n",
       "1       frame_7_484.jpg       1\n",
       "2        frame_8_87.jpg       1\n",
       "3       frame_7_112.jpg       0\n",
       "4       frame_7_388.jpg       0\n",
       "5       frame_7_152.jpg       0\n",
       "...                 ...     ...\n",
       "11975   frame_34_54.jpg       1\n",
       "11976   frame_35_79.jpg       0\n",
       "11977   frame_36_58.jpg       1\n",
       "11978  frame_37_112.jpg       0\n",
       "11979  frame_40_224.jpg       0\n",
       "\n",
       "[11793 rows x 2 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015a1ab7-cb60-4c73-9f27-709e109b95f6",
   "metadata": {},
   "source": [
    "*Добавляем в датафрейм новые кадры*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "807ac037-d797-4afb-8c64-093f7b2b980c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "image_dir = 'output_frames_1'\n",
    "image_files = os.listdir(image_dir)\n",
    "new_rows = []\n",
    "for image_file in image_files:\n",
    "        file_name = os.path.basename(image_file)\n",
    "        if file_name not in df_frames['file_name'].values:\n",
    "            new_row = {'file_name': file_name, 'labels': 0}\n",
    "            new_rows.append({'file_name': file_name, 'labels': 0})\n",
    "df_frames = pd.concat([df_frames, pd.DataFrame(new_rows)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f7397b85-8a71-4d5c-8982-89d587aebab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11980 entries, 0 to 11979\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   file_name  11980 non-null  object\n",
      " 1   labels     11980 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 187.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df_frames.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7c8a4a-e030-4e29-983f-ad3085cf5298",
   "metadata": {},
   "source": [
    "*Проставляем метки на кадры краж*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d5b8b733-5680-4292-9ab6-a76d31290377",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_label(number_video, start_frame, finish_frame):\n",
    "    for i in range(start_frame, finish_frame + 1):\n",
    "        df_frames.loc[df_frames['file_name'] == f\"frame_{number_video}_{i}.jpg\", 'labels'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "60b87497-b440-44da-9898-08887208aa50",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_label(13, 1, 460)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f68172-e076-4bc6-8c68-a52b2e25ee18",
   "metadata": {},
   "source": [
    "*Просмотров кадров*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "b35b2379-8c9c-4640-af71-43d20f639e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import time\n",
    "def show_video_frames(number_video):\n",
    "    path = 'frames'\n",
    "    images = [img for img in os.listdir(path) if img.endswith(\".jpg\") and f\"frame_{number_video}\" in img]\n",
    "    #images.sort()\n",
    "    \n",
    "    for img in images:\n",
    "        image_path = os.path.join(path, img)\n",
    "        image = Image.open(image_path)\n",
    "        image.show()\n",
    "        time.sleep(0.25)\n",
    "        image.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "bf5bce5a-30b9-43f3-bf28-7d04ed5b3813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: ylabel='Frequency'>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "df_frames['labels'].plot(kind='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ded8c824-a1a8-4f9a-942d-c3cf33b00943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3805"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df_frames['labels'] == 1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "aa67a7f4-1342-42ec-b5da-da3caf02c76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_frames_f.to_csv('metadata_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225d3b53-86fc-4c9a-a497-54e2d384a4d2",
   "metadata": {},
   "source": [
    "*Вспомогательные функции для обрезки по yolo боксам*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f56adff1-0cb3-4f27-b349-e9524574ff41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def crop_image(image, box):\n",
    "    additional_area = 1/5\n",
    "    x_min, y_min, x_max, y_max = box\n",
    "    width, height = image.size\n",
    "    box_width = x_max - x_min\n",
    "    box_height = y_max - y_min\n",
    "    x_min_2 = max(0, x_min - box_width*additional_area)\n",
    "    y_min_2 = max(0, y_min - box_height*additional_area)\n",
    "    x_max_2 = min(width, x_max + box_width*additional_area)\n",
    "    y_max_2 = min(height, y_max + box_height*additional_area)\n",
    "    area = (x_min_2, y_min_2, x_max_2, y_max_2)\n",
    "    cropped_img = image.crop(area)\n",
    "    return cropped_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eee1702f-dc97-447d-9d69-939bd3deaa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def recieve_box(image, frame):\n",
    "    model = YOLO('yolov8n.pt')\n",
    "    model.classes = [0]\n",
    "    results = model(source = image, classes=0, show = False, imgsz=640, conf=0.3, iou=0.4, save = False)\n",
    "    boxes = results[0].boxes.xyxy.cpu().numpy().astype(int)\n",
    "    cropped_images = []\n",
    "    for box in boxes:\n",
    "        cropped_images.append(crop_image(image, box))\n",
    "    save_dir = 'frames_cropped'\n",
    "    counter = 1\n",
    "    for im in cropped_images:\n",
    "        save_path = os.path.join(save_dir, f\"_cropped_{counter}.jpg\")\n",
    "        im.save(save_path)\n",
    "        counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ea0afd9e-a4c3-4d38-b71a-da1b7048376a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 7 persons, 51.6ms\n",
      "Speed: 12.1ms preprocess, 51.6ms inference, 8.7ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    }
   ],
   "source": [
    "path_to_image = 'frames/frame_30_106.jpg'\n",
    "image = Image.open(path_to_image)\n",
    "recieve_box(image, path_to_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca999a3-3ceb-495c-920f-62103e7c2721",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.utils.plotting import Annotator\n",
    "\n",
    "model = YOLO('yolov8n.pt')\n",
    "model.classes = [0]\n",
    "image_path = '/content/drive/MyDrive/yolo_dataset/test/images/cam-pos-video-five_0mABuXZe_mp4-36_jpg.rf.9f17c4aa766ded211ed3a70d1728403f.jpg'\n",
    "results = model(source = image_path, classes=0, show = False, imgsz=640, conf=0.3, iou=0.4, save = True)\n",
    "\n",
    "import os\n",
    "image_path = '/content/drive/MyDrive/yolo_dataset/test/images/cam-pos-video-five_0mABuXZe_mp4-36_jpg.rf.9f17c4aa766ded211ed3a70d1728403f.jpg'\n",
    "boxes = results[0].boxes.xyxy.cpu().numpy().astype(int)\n",
    "save_dir = 'runs/detect/predict18'\n",
    "for box in boxes:\n",
    "    x_min, y_min, x_max, y_max = box\n",
    "    #print(f\"Box coordinates: ({x_min}, {y_min}), ({x_max}, {y_max})\")\n",
    "    img = Image.open(image_path)\n",
    "    width, height = img.size\n",
    "    box_width = x_max - x_min\n",
    "    box_height = y_max - y_min\n",
    "    x_min_2 = max(0, x_min - box_width/3)\n",
    "    y_min_2 = max(0, y_min - box_height/3)\n",
    "    x_max_2 = min(width, x_max+ box_width/3)\n",
    "    y_max_2 = min(height, y_max + box_height/3)\n",
    "    area = (x_min_2, y_min_2, x_max_2, y_max_2)\n",
    "    cropped_img = img.crop(area)\n",
    "    save_path = os.path.join(save_dir, \"cropped_.jpg\")\n",
    "    cropped_img.save(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc174292-b30f-42aa-b795-1163be09fdfa",
   "metadata": {},
   "source": [
    "*Функция для трекинга человека на видео. Сохраняет видео с людьми в боксах и помечает их id*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8f7ca48-0152-4f18-9780-277b970fc7f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anastasia/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "def process_video_with_tracking(model, input_video_path, output_video_path):\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        raise Exception(\"Error: Could not open video file.\")\n",
    "\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        results = model.track(frame, iou=0.4, conf=0.25, persist=True, imgsz=608, verbose=False, tracker=\"bytetrack.yaml\", classes=0)\n",
    "    \n",
    "        if results[0].boxes.id != None: # this will ensure that id is not None -> exist tracks\n",
    "            boxes = results[0].boxes.xyxy.cpu().numpy().astype(int)\n",
    "            ids = results[0].boxes.id.cpu().numpy().astype(int)\n",
    "\n",
    "            for box, id in zip(boxes, ids):\n",
    "                additional_area = 1/10\n",
    "                x_min, y_min, x_max, y_max = box\n",
    "                width, height = frame_width, frame_height\n",
    "                box_width = x_max - x_min\n",
    "                box_height = y_max - y_min\n",
    "                x_min_2 = max(0, x_min - box_width*additional_area)\n",
    "                y_min_2 = max(0, y_min - box_height*additional_area)\n",
    "                x_max_2 = min(width, x_max + box_width*additional_area)\n",
    "                y_max_2 = min(height, y_max + box_height*additional_area)\n",
    "                area = (x_min_2, y_min_2, x_max_2, y_max_2)\n",
    "                cropped_img = Image.fromarray(frame, 'RGB').crop(area)\n",
    "                cropped_frame = frame[box[1]:box[3], box[0]:box[2]]\n",
    "                color = (0, 255, 255)\n",
    "                cv2.rectangle(frame, (box[0], box[1]), (box[2], box[3],), color, 2)\n",
    "                cv2.putText(\n",
    "                    frame,\n",
    "                    f\"Id {id}\",\n",
    "                    (box[0], box[1]),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.70,\n",
    "                    (0, 255, 255),\n",
    "                    2,\n",
    "                )\n",
    "\n",
    "        \n",
    "        out.write(frame)\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a56390-5eea-4600-bdb1-1da5c8332a12",
   "metadata": {},
   "source": [
    "*Функция, которая получает видео с n людьми в кадре и записывает в директорию n видео, где k-ое видео представляет собой исходное, обрезанное так, чтобы в кадре был только k-ый человек*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "809b6151-b96a-4c25-af3f-4f613b6a87d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "\n",
    "def process_video_individual_padding(model, input_video_path, output_dir):\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "    if not cap.isOpened():\n",
    "        raise Exception(\"Error: Could not open video file.\")\n",
    "\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    writers = {}\n",
    "    writers_sizes = {}\n",
    "    frame_count = 0\n",
    "    additional_area = 1/5\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        results = model.track(frame, iou=0.4, conf=0.3, persist=True, imgsz=608, verbose=False, tracker=\"bytetrack.yaml\", classes=0)\n",
    "        frame_count += 1\n",
    "\n",
    "        if hasattr(results[0].boxes, 'id') and results[0].boxes.id is not None:\n",
    "            boxes = results[0].boxes.xyxy.cpu().numpy().astype(int)\n",
    "            ids = results[0].boxes.id.cpu().numpy().astype(int)\n",
    "\n",
    "            for box, person_id in zip(boxes, ids):\n",
    "                x_min, y_min, x_max, y_max = box\n",
    "                pad_width = (x_max - x_min) * additional_area\n",
    "                pad_height = (y_max - y_min) * additional_area\n",
    "                x_min_pad = max(0, int(x_min - pad_width))\n",
    "                y_min_pad = max(0, int(y_min - pad_height))\n",
    "                x_max_pad = min(frame_width, int(x_max + pad_width))\n",
    "                y_max_pad = min(frame_height, int(y_max + pad_height))\n",
    "\n",
    "                cropped_frame = frame[y_min_pad:y_max_pad, x_min_pad:x_max_pad]\n",
    "                if person_id not in writers:\n",
    "                    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "                    output_size = (x_max_pad - x_min_pad, y_max_pad - y_min_pad)\n",
    "                    writer = cv2.VideoWriter(f\"{output_dir}/person_{person_id}.mp4\", fourcc, fps, output_size)\n",
    "                    writers[person_id] = writer\n",
    "                    writers_sizes[person_id] = output_size\n",
    "                    \n",
    "                else:\n",
    "                    desired_size = writers_sizes[person_id]\n",
    "                    resized_frame = cv2.resize(cropped_frame, desired_size)\n",
    "\n",
    "                    if resized_frame.size != 0:\n",
    "                        writers[person_id].write(resized_frame)\n",
    "\n",
    "        if frame_count % 100 == 0:\n",
    "            print(f\"Processed {frame_count} frames.\")\n",
    "\n",
    "\n",
    "    cap.release()\n",
    "    for writer in writers.values():\n",
    "        writer.release()\n",
    "\n",
    "    writers.clear()\n",
    "    writers_sizes.clear()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08df1d4-5000-402d-87e4-771e46aa1e1d",
   "metadata": {},
   "source": [
    "*Обрезаем видео по конкретному боксу(человека с id=track_id)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "94d5cfae-fb6b-458e-8dd2-96783072fd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "def process_single_track_video(model, input_video_path, output_video_path, track_id):\n",
    "    \n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "    if not cap.isOpened():\n",
    "        raise Exception(\"Error: Could not open video file.\")\n",
    "    \n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    \n",
    "    out = None\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        results = model.track(frame, iou=0.4, conf=0.3, persist=True, imgsz=608, verbose=False, tracker=\"bytetrack.yaml\", classes=0)\n",
    "        \n",
    "        if hasattr(results[0].boxes, 'id') and results[0].boxes.id != None:\n",
    "            boxes = results[0].boxes.xyxy.cpu().numpy().astype(int)\n",
    "            ids = results[0].boxes.id.cpu().numpy().astype(int)\n",
    "\n",
    "            for box, id in zip(boxes, ids):\n",
    "                if id == track_id:\n",
    "                    x_min, y_min, x_max, y_max = box\n",
    "                    if out is None:\n",
    "                        width = x_max - x_min\n",
    "                        height = y_max - y_min\n",
    "                        if width > 0 and height > 0:\n",
    "                            out = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
    "                            fixed_size = (int(width), int(height))\n",
    "                    try:\n",
    "                        x_min = max(x_min, 0)\n",
    "                        y_min = max(y_min, 0)\n",
    "                        x_max = min(frame_width, x_max)\n",
    "                        y_max = min(frame_height, y_max)\n",
    "                        cropped_frame = frame[y_min:y_max, x_min:x_max]\n",
    "                        if cropped_frame.shape[1] != fixed_size[0] or cropped_frame.shape[0] != fixed_size[1]:\n",
    "                            cropped_frame = cv2.resize(cropped_frame, (fixed_size[0], fixed_size[1]))\n",
    "                        out.write(cropped_frame)\n",
    "                    except Exception as e:\n",
    "                        print(\"Error managing frame size:\", e)\n",
    "    \n",
    "    cap.release()\n",
    "    if out is not None:\n",
    "        out.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb27b417-d1b7-49a9-a048-c50e108d200a",
   "metadata": {},
   "source": [
    "*examples of usages*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3766fee2-0ef2-408a-b23e-0a895f5021f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLOv8m summary (fused): 218 layers, 25886080 parameters, 0 gradients, 78.9 GFLOPs\n",
      "Processed 100 frames.\n"
     ]
    }
   ],
   "source": [
    "model = YOLO('yolov8n.pt')\n",
    "model.fuse()\n",
    "results = process_video_individual_padding(model, \"Shoplifting1.mp4\", output_dir=\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba5e011b-5fd0-4f94-ae03-ca2920145769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLOv8l summary (fused): 268 layers, 43668288 parameters, 0 gradients, 165.2 GFLOPs\n"
     ]
    }
   ],
   "source": [
    "model = YOLO('yolov8l.pt')\n",
    "model.fuse()\n",
    "results = process_video_with_tracking(model, \"Shoplifting13.mp4\", output_video_path=\"output/output_sample13_l.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "21fa00b9-a6c1-4208-8b95-8699e0a29803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLOv8n summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs\n"
     ]
    }
   ],
   "source": [
    "model = YOLO('yolov8n.pt')\n",
    "model.fuse()\n",
    "process_single_track_video(model, \"Shoplifting9.mp4\", \"output_sample9.mp4\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5735c743-21e0-4b6b-8d5c-c5cffbe5f63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_single_track_video(model, \"Shoplifting46.mp4\", \"output_sample46.mp4\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8b819c51-8c91-4819-84d8-6bdfdfc1dc1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLOv8m summary (fused): 218 layers, 25886080 parameters, 0 gradients, 78.9 GFLOPs\n",
      "Processing video: Shoplifting2.mp4\n",
      "Processing video: Shoplifting3.mp4\n",
      "Processing video: Shoplifting4.mp4\n",
      "Processing video: Shoplifting5.mp4\n",
      "Processing video: Shoplifting6.mp4\n",
      "Processing video: Shoplifting7.mp4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m src_directory \u001b[38;5;241m=\u001b[39m cwd\n\u001b[1;32m     23\u001b[0m res_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvideos_with_boxes\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 25\u001b[0m \u001b[43mprocess_all_videos\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mres_directory\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[49], line 16\u001b[0m, in \u001b[0;36mprocess_all_videos\u001b[0;34m(directory_path, output_directory)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing video: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvideo_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 16\u001b[0m     \u001b[43mprocess_video_with_tracking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_video_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to process \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvideo_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[43], line 22\u001b[0m, in \u001b[0;36mprocess_video_with_tracking\u001b[0;34m(model, input_video_path, output_video_path)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ret:\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miou\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpersist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m608\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbytetrack.yaml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(results[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mboxes, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m results[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mboxes\u001b[38;5;241m.\u001b[39mid \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;66;03m# this will ensure that id is not None -> exist tracks\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     boxes \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mboxes\u001b[38;5;241m.\u001b[39mxyxy\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/engine/model.py:479\u001b[0m, in \u001b[0;36mModel.track\u001b[0;34m(self, source, stream, persist, **kwargs)\u001b[0m\n\u001b[1;32m    477\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# batch-size 1 for tracking in videos\u001b[39;00m\n\u001b[1;32m    478\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrack\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 479\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/engine/model.py:439\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[0;32m--> 439\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/engine/predictor.py:168\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[0;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:35\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m---> 35\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/engine/predictor.py:248\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[0;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m--> 248\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39membed:\n\u001b[1;32m    250\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m [preds] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(preds, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m preds  \u001b[38;5;66;03m# yield embedding tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/engine/predictor.py:142\u001b[0m, in \u001b[0;36mBasePredictor.inference\u001b[0;34m(self, im, *args, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs inference on a given image using the specified model and arguments.\"\"\"\u001b[39;00m\n\u001b[1;32m    137\u001b[0m visualize \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    138\u001b[0m     increment_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_dir \u001b[38;5;241m/\u001b[39m Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mstem, mkdir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mvisualize \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type\u001b[38;5;241m.\u001b[39mtensor)\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    141\u001b[0m )\n\u001b[0;32m--> 142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/nn/autobackend.py:425\u001b[0m, in \u001b[0;36mAutoBackend.forward\u001b[0;34m(self, im, augment, visualize, embed)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_module:\n\u001b[0;32m--> 425\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;66;03m# TorchScript\u001b[39;00m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjit:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/nn/tasks.py:89\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/nn/tasks.py:107\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[0;34m(self, x, profile, visualize, augment, embed)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_augment(x)\n\u001b[0;32m--> 107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/nn/tasks.py:128\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[0;34m(self, x, profile, visualize, embed)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[0;32m--> 128\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[1;32m    129\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(x \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/nn/modules/block.py:232\u001b[0m, in \u001b[0;36mC2f.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    230\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv1(x)\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    231\u001b[0m y\u001b[38;5;241m.\u001b[39mextend(m(y[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mm)\n\u001b[0;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/nn/modules/conv.py:54\u001b[0m, in \u001b[0;36mConv.forward_fuse\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_fuse\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     53\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Perform transposed convolution of 2D data.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "def process_all_videos(directory_path, output_directory):\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "    model = YOLO('yolov8m.pt')\n",
    "    model.fuse()\n",
    "    for i in range(2, 49):\n",
    "        if i not in [42, 43, 44, 45]:\n",
    "            video_filename = f\"Shoplifting{i}.mp4\"\n",
    "            video_path = os.path.join(directory_path, video_filename)\n",
    "            if os.path.exists(video_path):\n",
    "                output_video_path = os.path.join(output_directory, f\"focused_{video_filename}\")\n",
    "                print(f\"Processing video: {video_filename}\")\n",
    "                try:\n",
    "                    process_video_with_tracking(model, video_path, output_video_path)\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to process {video_filename}: {str(e)}\")\n",
    "            else:\n",
    "                print(f\"File {video_filename} does not exist in the directory\")\n",
    "    \n",
    "src_directory = cwd\n",
    "res_directory = \"videos_with_boxes\"\n",
    "\n",
    "process_all_videos(src_directory, res_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1f9a45-9049-44b2-a671-ee8eed316dae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
